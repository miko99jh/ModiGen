import os
import time
import json
import torch
import concurrent.futures
from typing import Any, List
from llama_index.core.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM
from llama_index.core import PropertyGraphIndex, SimpleDirectoryReader, Settings, StorageContext
from llama_index.core.graph_stores import SimpleGraphStore
# Retrievers
from llama_index.core.retrievers import (
    BaseRetriever,
    PGRetriever,
    VectorContextRetriever,
    LLMSynonymRetriever
)

from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.llms.replicate.base import llm_completion_callback
from llama_index.core import StorageContext, load_index_from_storage
from llama_index.core.embeddings import resolve_embed_model
from llama_index.core.llms import (
    CustomLLM,
    CompletionResponse,
    CompletionResponseGen,
    LLMMetadata,
)

from llama_index.core.retrievers import BaseRetriever

from llama_index.core.retrievers import BaseRetriever
from typing import List, Any
from llama_index.core.base.query_pipeline.query import QueryComponent
from llama_index.core.schema import (
    NodeWithScore,
    QueryBundle
)

MAX_TIMEOUT = 60
temp = 0.3

class CustomRetriever(BaseRetriever):
    def __init__(
        self,
        vector_retriever: VectorContextRetriever,
        synonym_retriever: LLMSynonymRetriever,
        mode: str = "OR",  # Merge search results using the OR OR AND schema
    ) -> None:
        """Initializes parameters, sets vector searcher, synonym searcher, and merge mode"""
        self._vector_retriever = vector_retriever
        self._synonym_retriever = synonym_retriever
        if mode not in ("AND", "OR"):
            raise ValueError("Invalid mode.")
        self._mode = mode

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """Obtain nodes based on the query"""
        # Use vector and synonym searchers to search separately
        vector_nodes = self._vector_retriever.retrieve(query_bundle)
        synonym_nodes = self._synonym_retriever.retrieve(query_bundle)

        # Gets the ID of the node
        vector_ids = {n.node.node_id for n in vector_nodes}
        synonym_ids = {n.node.node_id for n in synonym_nodes}

        # Merge the search results into the dictionary by node ID
        combined_dict = {n.node.node_id: n for n in vector_nodes}
        combined_dict.update({n.node.node_id: n for n in synonym_nodes})

        # Decide how to select search results based on the merge mode
        if self._mode == "AND":
            retrieve_ids = vector_ids.intersection(synonym_ids)
        else:  # "OR" mode is used by default
            retrieve_ids = vector_ids.union(synonym_ids)

        # Returns the final retrieved node
        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]
        return retrieve_nodes

    def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """Returns the search result directly as a list of nodes"""
        results = self._retrieve(query_bundle)
        return results
    
class QwenCustomLLM(CustomLLM):
    context_window: int = 8192  # Context window size
    num_output: int = 2408  # The maximum number of tokens generated by the model at once
    model_name: str = "CodeLlama-7b-Instruct"  # Model name
    tokenizer: object = None  # Tokenizer
    model: object = None  # Model

    def __init__(self, pretrained_model_path):
        super().__init__()

        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_path,
            trust_remote_code=True,
            device_map="auto",
            torch_dtype=torch.float16,  # Use half-precision to save memory
        )

    @property
    def metadata(self) -> LLMMetadata:
        """Get LLM metadata."""
        return LLMMetadata(
            context_window=self.context_window,
            num_output=self.num_output,
            model_name=self.model_name,
        )

# @llm_completion_callback()  # Callback function
    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:
        print("complete function")

        # Encode input and send to the device where the model is located
        inputs = self.tokenizer(prompt, return_tensors='pt')
        input_ids = inputs["input_ids"].to("cuda")  # Extract input_ids and move them to GPU
        attention_mask = inputs["attention_mask"].to("cuda")  # Get attention_mask

        max_length = len(input_ids[0]) + 800
        # Use the model to generate output
        outputs = self.model.generate(
                input_ids,
                max_length=max_length,
                temperature=temp,
                do_sample=True,
                top_k=10,
                attention_mask=attention_mask,  # Pass in attention_mask
                pad_token_id=self.tokenizer.eos_token_id  # Set pad_token_id
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return CompletionResponse(text=response)
    
    # @llm_completion_callback()
    def stream_complete(
        self, prompt: str, **kwargs: Any
    ) -> CompletionResponseGen:
        print("stream completion function")

        # The coded input is sent to the device where the model is located
        inputs = self.tokenizer(prompt, return_tensors='pt')
        input_ids = inputs["input_ids"].to("cuda")
        attention_mask = inputs["attention_mask"].to("cuda")
        
        max_length = len(input_ids[0]) + 500
        # Generate output using the model (custom stream generation logic required)
        outputs = self.model.generate(
                input_ids,
                max_length=max_length,
                temperature=temp,
                do_sample=True,
                top_k=10,
                attention_mask=attention_mask,
                pad_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        for token in response:
            yield CompletionResponse(text=token, delta=token)

def read_json(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

def read_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def write_file(file_path, content):
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(content)

# Integrate the process_instruction function into the original code
def process_instruction(idx, combined_query, output_path, query_engine):
    try:
        print(f"Processing instruction {idx+1}...")

        # Use a thread pool to execute the query and set a timeout
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(query_engine.query, combined_query)
            response_rag = future.result(timeout=MAX_TIMEOUT)  # Set a timeout limit

        # Extract generated text
        generated_text = response_rag.response
        response_text = generated_text[len(combined_query):].strip()  # Remove the prompt text from the generated text
        # Write the generation result to file
        write_file(output_path, response_text)
        print(f"Processed instruction {idx+1}, output saved to {output_path}")

    except concurrent.futures.TimeoutError:
        # Exceeded the maximum timeout, using backup logic
        print(f"Timeout processing instruction {idx+1}, using backup method...")
        response = Settings.llm.complete(prompt=combined_query)
        write_file(output_path, response.text)
        print(f"Processed instruction {idx+1}, output saved to {output_path} using backup method.")

    except Exception as e:
        # Other exception handling
        print(f"Error processing instruction {idx+1}: {e}")
        response = Settings.llm.complete(prompt=combined_query)
        write_file(output_path, response.text)
        print(f"Processed instruction {idx+1}, output saved to {output_path} after error.")

def main():
    total_start_time = time.time()
    current_folder = os.path.dirname(os.path.abspath(__file__))
    # Configure paths and parameters
    model_size = 7
    mode = 'model'
    context_file = os.path.join(current_folder, f'context/0_Prompt_{mode}.txt')
    output_dir = os.path.join(current_folder, f'generation_example/{mode}_result/codellama/output-{model_size}b-Grag')
    json_file_path = os.path.join(current_folder, f'json_files/test_6_libraries_new.json')

    # Load context and instructions
    context_text = read_file(context_file)
    data = read_json(json_file_path)
    instructions = [item['instruction'] for item in data[f'{mode}']]

    # Embed the model using BAAI/ BURGE-large-en-v1.5
    Settings.embed_model = resolve_embed_model("local:/xxx/GRAG/BAAI/bge-large-en-v1.5")
    # Initialize QwenCustomLLM
    model_dir = "/xxx/LLaMA-Factory/LLMs/CodeLlama-7b-Instruct-hf" # Replace with the path to the model directory
    Settings.llm = QwenCustomLLM(model_dir)

    documents = SimpleDirectoryReader("./GRAG/docs").load_data()

    try:
        # Attempts to persist the existing storage_context
        storage_context = StorageContext.from_defaults(persist_dir="./GRAG/storage_Pgraph_modelica_all-4")
        # load index
        pg_index = load_index_from_storage(storage_context)
        print("Storage context persisted successfully.")
    except Exception as e:
        print(f"Persisting storage context failed: {e}")
        # If persistence fails, re-create the PropertyGraphIndex
        print("Rebuilding PropertyGraphIndex...")
        # Build graph stores context and index
        space_name = "modelica"
        graph_store = SimpleGraphStore(space_name=space_name, edge_types=["relationship"], tags=["entity"])
        storage_context = StorageContext.from_defaults(graph_store=graph_store)
        pg_index = PropertyGraphIndex.from_documents(
            documents=documents,
            storage_context=storage_context,
            show_progress=True,
            max_triplets_per_chunk=10,
            include_embeddings=True,
        )
        pg_index.storage_context.persist("./storage_Pgraph_modelica_all")
        print("New storage context persisted successfully.")
    
    query_engine = pg_index.as_query_engine(similarity_top_k=1)

    qa_prompt_tmpl_str = (
        '''These are examples of modelica code generation. 
        Focus on the contents which related to the query and ignore irrelevant information.\n'''
        "---------------------\n"
        "{context_str}\n"
        "---------------------\n"
        '''Focus on the source of the library and components being used.
        Query: {query_str}\n
        Note: You only need to output valid and well-structured Modelica scripts.\n'''
    )

    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)
    query_engine.update_prompts(
        {"response_synthesizer:text_qa_template": qa_prompt_tmpl})

    # Ensure that the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Generation
    for run_number in range(1, 11):
        run_start_time = time.time()
        for idx, instruction in enumerate(instructions):
            prompt_text = instruction
            combined_query = context_text + "\n" + prompt_text
            combined_query = prompt_text

            output_file_name = f"{mode}_{idx+1}_response_{run_number}.txt"
            output_path = os.path.join(output_dir, output_file_name)

            if os.path.exists(output_path):
                print(f"File {output_file_name} already exists. Skipping this prompt.")
                continue

            process_instruction(idx, combined_query, output_path, query_engine)

        run_end_time = time.time()
        print(f"Run {run_number} time taken: {(run_end_time - run_start_time)/3600} hours")

    print("All tasks completed successfully!")
    total_end_time = time.time()
    print(f"Total time taken: {(total_end_time - total_start_time)/3600} hours")

if __name__ == "__main__":
    main()